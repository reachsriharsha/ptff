include:
  - docker-compose.networks.yaml
  - docker-compose.milvus.yaml

services:
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - backend
      - frontend
    networks:
      - ptff_network
  
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
    networks:
      - ptff_network

  postgres:
    image: postgres:17-alpine
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: ptffdb
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - ptff_network

  backend:
    build:
      context: ./be
      dockerfile: Dockerfile
    environment:
      - PORT=8000
      - REDIS_URL = redis://redis:6379/0
    env_file:
      - ./be/.env.prod
    depends_on:
      - redis
      - postgres
    volumes:
      - ./be:/app
      - ./uploads/be:/app/uploads
    networks:
      - ptff_network

  celery_worker:
    build:
      context: ./be
      dockerfile: Dockerfile
    command: celery -A tasks worker --loglevel=info
    depends_on:
      - redis
      - backend
    networks:
      - ptff_network

  frontend:
    build:
      context: ./fe
      dockerfile: Dockerfile
    environment:
      - BACKEND_URL=http://nginx/api
    env_file:
      - ./fe/.env.prod
    volumes:
      - ./fe:/app
      - ./uploads/fe:/app/uploads
    networks:
      - ptff_network

  ollama:
    image: ollama/ollama:latest  # Use the latest Ollama image
    container_name: ollama 
    ports:
      - "11434:11434"  # Expose the Ollama API port
    volumes:
      - ./models:/root/.local/share/ollama/models  # Mount your model files 
    environment:
      - GPU_MODE=auto  # Enable GPU support if available
      - GIN_MODE=release
    restart: unless-stopped 
volumes:
  postgres_data: